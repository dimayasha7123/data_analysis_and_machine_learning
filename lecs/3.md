### [Список вопросов](main.md)

# Предварительная обработка и очистка данных. Обработка категориальных переменных. Устранение выбросов.

Хорошо, когда данные подготовлены таким образом, чтобы структура решаемой задачи максимально точно считывалась алгоритмами моделирования, которыми вы будете пользоваться. В целом рекомендуется представлять данные в разном виде и формах, а затем на каждом из них испытывать работу алгоритма. Это поможет вам выяснить, какие преобразования данных помогут представить структуру решаемой задачи в целом.

## Стандартизация

**Стандартизация** – полезный способ преобразования атрибутов с распределением Гаусса и различными средними и стандартными отклонениями в стандартное распределение Гаусса со средним значением (mean) 0 и стандартным отклонением (stdev) 1.

Он больше всего необходим для методов, которые принимают распределение Гаусса во входных переменных и лучше работают с масштабированными данными, такими как **линейная регрессия, логистическая регрессия и линейный анализ дискриминации**.

Также многие другие элементы, используемые в целовой функции алгоритма обучения (**ядро ​​RBF машин опорных векторов** или **регуляризаторы l1 и l2** линейных моделей) предполагают, что все функции сосредоточены вокруг нуля и имеют дисперсию в том же порядке. Если характеристика имеет дисперсию, которая на порядки больше, чем у других, она может доминировать над целевой функцией и сделать оценщик неспособным правильно учиться на других функциях, как ожидалось.

**Что использовать:** StandardScaler

``` python
from sklearn.preprocessing import StandardScaler
# X - входные данные
scaler = StandardScaler().fit(X)
rescaledX = scaler.transform(X)
# далее тоже самое, но функции нормализации другие
```

## Масштабирование

Альтернативная стандартизация — это **масштабирование** функций таким образом, чтобы они находились между заданным минимальным и максимальным значением, часто между нулем и единицей, или так, чтобы максимальное абсолютное значение каждой функции масштабировалось до размера единицы. Этого можно добиться с помощью **MinMaxScaler** или **MaxAbsScaler** соответственно.

Если датасет содержит атрибуты с разной шкалой, лучше применить масштабирование, так как многим алгоритмам машинного обучения удобно работать с атрибутами одинакового масштаба. Так все атрибуты будут иметь одинаковый вес при работе алгоритма.

Мотивация к использованию этого масштабирования включает устойчивость к очень небольшим стандартным отклонениям функций и сохранение нулевых записей в разреженных данных (также масштабирование сохранит разреженную структуру данных, чего не сделает стандартизация, так как данные будут центрированы).

Это полезно для алгоритмов оптимизации, используемых в главных алгоритмах машинного обучения, таких как **градиентный спуск**. Это также полезно для алгоритмов, которые определяют вес входных данных, например, **алгоритм регрессии** и **нейронные сети**, и для алгоритмов, использующих дистанционные методы, такие как **метод k-ближайших соседей**.


**Что использовать:** MinMaxScaler или MaxAbsScaler.

## Нормализация

**Нормализация** в scikit-learn означает масштабирование каждого наблюдения (строки) в длину 1 (в линейной алгебре ее называют единичной нормой или вектором с длиной 1). Этот метод предварительной обработки может быть полезен для разреженных датасетов (с множествомнулей) с атрибутами разной шкалы при использовании алгоритмов, которые определяют вес входных данных, такие как нейронные сети и алгоритмы, которые используют дистанционные методы, такие как метод k-ближайших соседей.

**Что использовать:** Normalizer.

## Бинаризация

Вы можете преобразовать свои данные с помощью бинарного порога. Все значения выше порога отмечаются 1, а все равные или или ниже порога обозначаются как 0. Это называется **бинаризацией** (binarizing). Она может быть полезно, если вы хотите вероятные значения превратить в четкие, или если вы конструируете признаки и хотите добавить новые, имеющие
какое-то значение.

**Что использовать:** Binarizer.

## Очистка данных

**Устранение дублирования записей.** В базе данных клиентов некоторые клиенты могут быть представлены несколькими записями, хотя во многих случаях это результат небрежности или следствие того, что, например, клиенты перемещаются
с одного места на другое без извещения об изменении адреса. Или же могут существовать две одинаковых записи об одном клиенте с допущенной ошибкой в имени в одной из них.

**Недостаток области совместимости**. Записи с дефолтными аномальными значениями, например дата рождения 1 января 1900 года. Вряд-ли это так, но на алгоритм эти данные окажут сильное влияние. Или же много людей заполняет дату рождения как 11.11.11, например, если не хотят ее выдавать, надо, чтобы такие значения учитывались как незаполненные.

И другое: типографские ошибки.

## Немного про кодирование данных

- **Адрес.** Точный адрес не всегда нужен и его можно преобразовать в регион.
- **Возраст.** Преобразовываем в возраст. Можно выделить классы, например на каждые 5 лет.
- **Доход/кредит.** Точную сумму можно разделить на 1000, или опять выделить в интеральваные классы.
- **Да/нет.** Преобразуем в 1/0.
- **Дата приобретения.** Можно преобразовать в число месяцев, особенно если товар обновляется ежемесячно, напрмиер, журналы.
- **Один ко многим...** Например, если люди, которые покупают журналы, и один человек может купить несколько журналов. Записи в таблице с атрибутом человека и одного журнала (атрибут - название журнала) будет сложно обрабатывать. Необходимо завести по атрибуту на каждый журнал и указывать там бинарное значение, которое показывает, является ли человек подписчиком журнала.

## Обработка категориальных переменных

Часто характеристики задаются не как непрерывные значения, а как категориальные. Такие функции могут быть эффективно закодированы как целые числа.

Можем использовать **OrdinalEncoder**. Этот оценщик преобразует каждую категориальную характеристику в одну новую характеристику целых чисел (от 0 до n_categories — 1). Такое целочисленное представление не может использоваться напрямую со всеми оценщиками scikit-learn, поскольку они ожидают непрерывного ввода и интерпретируют категории как упорядоченные, что часто нежелательно (например, может получиться что набор браузеров был упорядочен произвольно, хотя это не имеет смысла).

Еще одна возможность преобразовать категориальные функции в функции — это использовать кодировку «один из K», также известную как одноразовое или фиктивное кодирование. Этот тип кодирования может быть получен с помощью **OneHotEncoder**, который преобразует каждый категориальный признак с n_categories возможными значениями в n_categories двоичные признаки, один из которых равен 1, а все остальные 0.

## Выбросы

По разным причинам многие наборы данных реального мира содержат пропущенные значения, часто закодированные как пробелы, NaN или другие заполнители. Однако такие наборы данных несовместимы с оценщиками scikit-learn, которые предполагают, что все значения в массиве являются числовыми, и что все они имеют и имеют значение. **Основная стратегия** использования неполных наборов данных — **отбрасывать** целые строки и/или столбцы, содержащие пропущенные значения. Однако это происходит ценой потери данных, которые могут быть ценными (хотя и неполными). **Лучшая стратегия** — это **подменять** недостающие значения, т.е. получить их из известной части данных.

Одним из типов алгоритма восстановления является одномерный, который подменяет значения в i-м измерении признака, используя только не пропущенные значения в этом же измерении признака (например, **impute.SimpleImputer**). Напротив, многомерные алгоритмы восстановления используют весь набор доступных измерений характеристик для оценки недостающих значений (например, **impute.IterativeImputer**).

Класс **SimpleImputer** предоставляет основные стратегии для восстановления отсутствующих значений. Пропущенные значения могут быть восстановлены с использованием предоставленного постоянного значения или с использованием статистики (среднего, медианного или наиболее частого) каждого столбца, в котором находятся пропущенные значения. Также поддерживаются категориальные данные (при использовании 'most_frequent' или 'constant' стратегии).
